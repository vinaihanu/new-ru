# -*- coding: utf-8 -*-
"""case.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d_hzn4Y_CH5ocBhlqYDuQSQXyBUF0Y1i
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import torch.nn.functional as F

# Custom Dataset class (replace with your data loading logic)
class AnomalyDataset(Dataset):
    def __init__(self, data, labels=None):
        self.data = torch.FloatTensor(data)
        self.labels = labels  # Optional, for evaluation

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx] if self.labels is not None else self.data[idx]

# Variational Autoencoder for Anomaly Detection
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, latent_dim=32):
        super(VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
            # Removed nn.Sigmoid() as data is not necessarily [0,1] after StandardScaler
        )

    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar

# VAE Loss function
def vae_loss(recon_x, x, mu, logvar):
    # Use MSE for reconstruction error when data is not bounded [0,1]
    # BCE is for binary classification or data strictly between 0 and 1
    # MSE expects input and target to have similar value ranges
    MSE_recon = F.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return MSE_recon + KLD

# Training function
def train_vae(model, dataloader, epochs=50, lr=1e-3):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.train()
    losses = []

    for epoch in range(epochs):
        train_loss = 0
        for batch_idx, (data, _) in enumerate(dataloader):
            optimizer.zero_grad()
            recon_batch, mu, logvar = model(data)
            loss = vae_loss(recon_batch, data, mu, logvar)
            loss.backward()
            train_loss += loss.item()
            optimizer.step()

        avg_loss = train_loss / len(dataloader.dataset)
        losses.append(avg_loss)
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')

    return losses

# Anomaly scoring using reconstruction error
def get_reconstruction_error(model, data):
    model.eval()
    with torch.no_grad():
        recon, _, _ = model(data)
        error = torch.mean((recon - data)**2, dim=1)
    return error.numpy()

# Example usage (replace with your assembly line data, e.g., ultrasound sensor readings)
# Assume data is 2D array of shape (n_samples, n_features)

# Generate synthetic normal data for demo (replace with real data)
np.random.seed(42)
normal_data = np.random.normal(0, 1, (1000, 20))  # 1000 normal samples, 20 features
anomalous_data = np.random.normal(5, 2, (100, 20))  # 100 anomalous samples

# Combine and normalize (train only on normal data!)
scaler = StandardScaler()
normal_data_scaled = scaler.fit_transform(normal_data)
normal_dataset = AnomalyDataset(normal_data_scaled)
train_loader = DataLoader(normal_dataset, batch_size=32, shuffle=True)

# Test data (normal + anomalous)
test_data = np.vstack([normal_data, anomalous_data])
test_data_scaled = scaler.transform(test_data)
test_dataset = AnomalyDataset(test_data_scaled, labels=np.array([0]*1000 + [1]*100))
test_loader = DataLoader(test_dataset, batch_size=32)

# Initialize and train VAE
input_dim = normal_data_scaled.shape[1]
model = VAE(input_dim)
losses = train_vae(model, train_loader, epochs=50)

# Detect anomalies
test_errors = get_reconstruction_error(model, test_dataset.data)
threshold = np.percentile(test_errors[:1000], 95)  # 95th percentile of normal errors
predictions = (test_errors > threshold).astype(int)

# Results
print(f"Threshold: {threshold:.4f}")
print(f"Anomalies detected: {np.sum(predictions)} out of {len(predictions)}")
print(f"Accuracy: {np.mean(predictions == test_dataset.labels):.4f}")

# Plot losses and errors
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(1, 2, 2)
plt.hist(test_errors[:1000], bins=50, alpha=0.7, label='Normal')
plt.hist(test_errors[1000:], bins=50, alpha=0.7, label='Anomalous')
plt.axvline(threshold, color='r', linestyle='--', label='Threshold')
plt.title('Reconstruction Errors')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.legend()
plt.show()